{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n",
      "pandas version: 1.0.1\n",
      "matplotlib version: 3.1.3\n",
      "NumPy version: 1.18.1\n",
      "SciPy version: 1.4.1\n",
      "IPython version: 7.12.0\n",
      "scikit-learn version: 0.22.1\n",
      "seaborn version: 0.10.0\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# This block is from https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "#load packages\n",
    "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np #foundational package for scientific computing\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import sklearn #collection of machine learning algorithms\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "import seaborn as sns #collection of functions for data visualization\n",
    "print(\"seaborn version: {}\". format(sns.__version__))\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder #OneHot Encoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "%matplotlib inline\n",
    "\n",
    "#misc libraries\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "test_raw = pd.read_csv('../input/Corona_NLP_test.csv')\n",
    "train_raw = pd.read_csv('../input/Corona_NLP_train_utf8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>44955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName             Location     TweetAt  \\\n",
       "0         1       44953                  NYC  02-03-2020   \n",
       "1         2       44954          Seattle, WA  02-03-2020   \n",
       "2         3       44955                  NaN  02-03-2020   \n",
       "3         4       44956          Chicagoland  02-03-2020   \n",
       "4         5       44957  Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative  \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive  \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive  \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative  \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41157 entries, 0 to 41156\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   UserName       41157 non-null  int64 \n",
      " 1   ScreenName     41157 non-null  int64 \n",
      " 2   Location       32567 non-null  object\n",
      " 3   TweetAt        41157 non-null  object\n",
      " 4   OriginalTweet  41157 non-null  object\n",
      " 5   Sentiment      41157 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "train_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3798 entries, 0 to 3797\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   UserName       3798 non-null   int64 \n",
      " 1   ScreenName     3798 non-null   int64 \n",
      " 2   Location       2964 non-null   object\n",
      " 3   TweetAt        3798 non-null   object\n",
      " 4   OriginalTweet  3798 non-null   object\n",
      " 5   Sentiment      3798 non-null   object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 178.2+ KB\n"
     ]
    }
   ],
   "source": [
    "test_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We could extract a lot of information from this data via visualization, but for the purpose of this project, we want to focus on building a sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = train_raw.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1)\n",
    "test_clean = test_raw.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n",
       "1  advice Talk to your neighbours family to excha...            Positive\n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive\n",
       "3  My food stock is not the only one which is emp...            Positive\n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean.iloc[1]['OriginalTweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#borrowed from the Text Classification Kaggle course\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
    "textcat = nlp.create_pipe(\n",
    "              \"textcat\",\n",
    "              config={\n",
    "                \"architecture\": \"bow\"})\n",
    "\n",
    "# Add the TextCategorizer to the empty model\n",
    "nlp.add_pipe(textcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_clean.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lable in labels:\n",
    "    textcat.add_label(lable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutral', 'Positive', 'Extremely Negative', 'Negative',\n",
       "       'Extremely Positive'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_clean['OriginalTweet'].values\n",
    "train_labels = [{'cats': {'Neutral': label == 'Neutral',\n",
    "                          'Positive': label == 'Positive',\n",
    "                         'Extremely Negative': label == 'Extremely Negative',\n",
    "                         'Negative': label == 'Negative',\n",
    "                         'Extremely Positive': label == 'Extremely Positive',}} \n",
    "                for label in train_clean['Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(train_texts, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch\n",
    "import random\n",
    "\n",
    "#borrowed from the Kaggle TextCatgorization course \n",
    "def train_cat (nlp_model, data, b_size, ep):\n",
    "    random.seed(1)\n",
    "    spacy.util.fix_random_seed(1)\n",
    "    optimizer = nlp_model.begin_training()\n",
    "    \n",
    "    losses = {}\n",
    "    for epoch in range(ep):\n",
    "        random.shuffle(data)\n",
    "        # Create the batch generator with batch size = 8\n",
    "        batches = minibatch(data, size=b_size)\n",
    "        # Iterate through minibatches\n",
    "        for batch in batches:\n",
    "            # Each batch is a list of (text, label) but we need to\n",
    "            # send separate lists for texts and labels to update().\n",
    "            # This is a quick way to split a list of tuples into lists\n",
    "            texts, labels = zip(*batch)\n",
    "            nlp_model.update(texts, labels, sgd=optimizer, losses=losses)\n",
    "        print(losses)\n",
    "    \n",
    "    return nlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'textcat': 28.74246430164203}\n",
      "{'textcat': 50.08783368370496}\n",
      "{'textcat': 67.78798086429015}\n"
     ]
    }
   ],
   "source": [
    "nlp_cat_trained = train_cat (nlp, train_data, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20689538 0.18483678 0.16582394 0.07810719 0.02132244]\n",
      " [0.04228055 0.6522597  0.00823701 0.08055886 0.43530792]\n",
      " [0.07611483 0.49128523 0.03388204 0.3087234  0.555106  ]]\n"
     ]
    }
   ],
   "source": [
    "#let's see how well our model performs\n",
    "texts = test_clean.OriginalTweet[:3].tolist()\n",
    "docs = [nlp.tokenizer(text) for text in texts]\n",
    "    \n",
    "# Use textcat to get the scores for each doc\n",
    "textcat = nlp.get_pipe('textcat')\n",
    "scores, _ = textcat.predict(docs)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neutral', 'Positive', 'Extremely Positive']\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = scores.argmax(axis=1)\n",
    "print([textcat.labels[label] for label in predicted_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Extremely Negative\n",
       "1              Positive\n",
       "2    Extremely Positive\n",
       "Name: Sentiment, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean.Sentiment[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "We extracted the frist three tweets from the test data, and two out of three right, which isn't as good as we expected, let's do a proper evaluation of the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_results (model,test_texts):\n",
    "    docs = [nlp.tokenizer(text) for text in test_texts]\n",
    "    textcat = model.get_pipe('textcat')\n",
    "    scores, _ = textcat.predict(docs)\n",
    "    predicted_labels = scores.argmax(axis=1)\n",
    "    results = [textcat.labels[label] for label in predicted_labels]\n",
    "    return results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred_results (nlp,test_clean.OriginalTweet.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_clean.Sentiment.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_score = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5284360189573459"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "The accuracy score is only slightly above 50%, which isn't that goodd, this could be due to the 'bow' architecture we are using for our model, for efficientcy, let's try a better architecture. Based on the spaCy document, seems like 'ensemble' is the best architecture, let's give it a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.blank(\"en\")\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
    "textcat_en = nlp.create_pipe(\n",
    "              \"textcat\",\n",
    "              config={\n",
    "                \"architecture\": \"ensemble\"})\n",
    "\n",
    "# Add the TextCategorizer to the empty model\n",
    "nlp_en.add_pipe(textcat_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lable in labels:\n",
    "    textcat_en.add_label(lable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'textcat': 25.404834992950782}\n",
      "{'textcat': 41.282424603850814}\n",
      "{'textcat': 52.998172056817566}\n"
     ]
    }
   ],
   "source": [
    "nlp_en_cat = train_cat (nlp_en, train_data, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_en = pred_results (nlp_en_cat,test_clean.OriginalTweet.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_score_en = accuracy_score(y_test, y_pred_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6977356503422855"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_score_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "The ensemble architecture took a long time to run, but the result was great, over 20% improvement, with an overall score of 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document vectors and standard ML models approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_lg==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.3.1/en_core_web_lg-2.3.1.tar.gz#egg=en_core_web_lg==2.3.1 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from en_core_web_lg==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (4.47.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (0.9.6)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\wesu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (3.0.4)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# Need to load the large model to get the vectors\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Need to load the large model to get the vectors\n",
    "nlp_lg = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for faster performance \n",
    "with nlp_lg.disable_pipes():\n",
    "    doc_vectors = np.array([nlp_lg(text).vector for text in train_clean.OriginalTweet])\n",
    "    X_test_vectors = np.array([nlp_lg(text).vector for text in test_clean.OriginalTweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(doc_vectors, train_clean.Sentiment,\n",
    "                                                    test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n",
    "\n",
    "#Common Model Algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model (X_tr, y_tr):\n",
    "    #this is from https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n",
    "    #Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "    MLA = [\n",
    "        #Ensemble Methods\n",
    "        ensemble.GradientBoostingClassifier(),\n",
    "        ensemble.RandomForestClassifier(),\n",
    "\n",
    "\n",
    "        #GLM\n",
    "        linear_model.LogisticRegressionCV(),\n",
    "\n",
    "\n",
    "        #SVM\n",
    "        svm.SVC(probability=True),\n",
    "        svm.LinearSVC(),\n",
    "\n",
    "        XGBClassifier()\n",
    " \n",
    "        ]\n",
    "    \n",
    "    #split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "    #note: this is an alternative to train_test_split\n",
    "    #cv_split = model_selection.ShuffleSplit(test_size = .2, train_size = .8, random_state = 0 ) # run model 10x with 80/20 split intentionally leaving out 10%\n",
    "\n",
    "    #create table to compare MLA metrics\n",
    "    MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "    MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "    #create table to compare MLA predictions\n",
    "    MLA_predict = y_tr\n",
    "\n",
    "    #index through MLA and save performance to table\n",
    "    row_index = 0\n",
    "    for alg in MLA:\n",
    "\n",
    "        #set name and parameters\n",
    "        MLA_name = alg.__class__.__name__\n",
    "        #print(MLA_name)\n",
    "        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "        MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "        #print(y_tr.shape)\n",
    "\n",
    "        #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "        cv_results = model_selection.cross_validate(alg, X_tr, y_tr, cv = 5, scoring='accuracy', return_train_score=True)\n",
    "        \n",
    "\n",
    "        MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "        MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "        MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "        #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "        MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "\n",
    "\n",
    "        #save MLA predictions - see section 6 for usage\n",
    "        #alg.fit(X_tr, y_tr)\n",
    "        #MLA_predict[MLA_name] = alg.predict(X_tr)\n",
    "\n",
    "        row_index+=1\n",
    "\n",
    "\n",
    "    #print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "    MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "    return MLA_compare\n",
    "    #MLA_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = base_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLA Name</th>\n",
       "      <th>MLA Parameters</th>\n",
       "      <th>MLA Train Accuracy Mean</th>\n",
       "      <th>MLA Test Accuracy Mean</th>\n",
       "      <th>MLA Test Accuracy 3*STD</th>\n",
       "      <th>MLA Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC</td>\n",
       "      <td>{'C': 1.0, 'break_ties': False, 'cache_size': ...</td>\n",
       "      <td>0.523325</td>\n",
       "      <td>0.487406</td>\n",
       "      <td>0.0146761</td>\n",
       "      <td>2369.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegressionCV</td>\n",
       "      <td>{'Cs': 10, 'class_weight': None, 'cv': None, '...</td>\n",
       "      <td>0.50548</td>\n",
       "      <td>0.482843</td>\n",
       "      <td>0.0113096</td>\n",
       "      <td>75.4839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': True,...</td>\n",
       "      <td>0.493872</td>\n",
       "      <td>0.467455</td>\n",
       "      <td>0.0088838</td>\n",
       "      <td>57.2498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'criterion': 'friedman_mse'...</td>\n",
       "      <td>0.544451</td>\n",
       "      <td>0.433627</td>\n",
       "      <td>0.0133705</td>\n",
       "      <td>1686.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'base_score': 0.5, 'booster': 'gbtree', 'cols...</td>\n",
       "      <td>0.501201</td>\n",
       "      <td>0.421965</td>\n",
       "      <td>0.0208606</td>\n",
       "      <td>238.867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.412705</td>\n",
       "      <td>0.0177953</td>\n",
       "      <td>51.1937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     MLA Name  \\\n",
       "3                         SVC   \n",
       "2        LogisticRegressionCV   \n",
       "4                   LinearSVC   \n",
       "0  GradientBoostingClassifier   \n",
       "5               XGBClassifier   \n",
       "1      RandomForestClassifier   \n",
       "\n",
       "                                      MLA Parameters MLA Train Accuracy Mean  \\\n",
       "3  {'C': 1.0, 'break_ties': False, 'cache_size': ...                0.523325   \n",
       "2  {'Cs': 10, 'class_weight': None, 'cv': None, '...                 0.50548   \n",
       "4  {'C': 1.0, 'class_weight': None, 'dual': True,...                0.493872   \n",
       "0  {'ccp_alpha': 0.0, 'criterion': 'friedman_mse'...                0.544451   \n",
       "5  {'base_score': 0.5, 'booster': 'gbtree', 'cols...                0.501201   \n",
       "1  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...                0.999946   \n",
       "\n",
       "  MLA Test Accuracy Mean MLA Test Accuracy 3*STD MLA Time  \n",
       "3               0.487406               0.0146761  2369.49  \n",
       "2               0.482843               0.0113096  75.4839  \n",
       "4               0.467455               0.0088838  57.2498  \n",
       "0               0.433627               0.0133705  1686.61  \n",
       "5               0.421965               0.0208606  238.867  \n",
       "1               0.412705               0.0177953  51.1937  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  9.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.4172132418597161\n"
     ]
    }
   ],
   "source": [
    "#I have been getting good results from RF model, though it didnt score the best in the above table, we will still use it to test how much imporvement we could get from doing hypterparameter tunning\n",
    "rf = ensemble.RandomForestClassifier(random_state = 1)\n",
    "param_grid =  {'n_estimators': [400,450,500,550],\n",
    "               'criterion':['gini','entropy'],\n",
    "                                  'bootstrap': [True],\n",
    "                                  'max_depth': [15, 20, 25],\n",
    "                                  'max_features': ['auto','sqrt', 10],\n",
    "                                  'min_samples_leaf': [2,3],\n",
    "                                  'min_samples_split': [2,3]}\n",
    "#GS took too long, going to use rnd search instead                                  \n",
    "#gs_cv_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "\n",
    "rs_rf = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 5, cv = 2, verbose = True, n_jobs = -1)\n",
    "best_rf = rs_rf.fit(X_train,y_train)\n",
    "print('Best Score: ' + str(best_rf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 550,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 15,\n",
       " 'criterion': 'entropy',\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "The result is worst than the default model. For the sake of completing this analysis, we will run an evaluation against the SVC model, which the top model in our base model anlaysis. However, we don't expect this will be better than the spaCy text pipe approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_clean.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_ml = svm.SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_ml = svm_ml.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_ml.predict(X_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "a_score_v = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47130068457082674"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_score_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "The score is slightly better than the one we observed in the base model analysis for the SVM model, but still much worst than the spaCy model using the ensemble architecture. In a way, we can conclude that vectorization of docs is less accurate when compared to the spaCy model with an ensemble architecture that uses a combination of bow and CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
